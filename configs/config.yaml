hyper_parameter:
  learning_rate: 0.0001  # 1e-4 -> 1.0e-4 for float representation
  clip_len: 32  # 32 -> 16으로 줄여서 메모리 사용량 감소
  ratio_ssi: 1
  ratio_tgm: 10
  ratio_ssi_image : 0.5
  epochs: 100
  patient: 5
  batch_size: 4  # 4 -> 2로 줄여서 메모리 사용량 감소
  update_frequency: 6  # N 프레임마다 가중치 업데이트 (스트리밍용)
  
  # Self-Forcing parameters
  # use_self_forcing: true
  # self_forcing_ratio: 0.3  # 30% 확률로 self-forcing 적용
  # self_forcing_start_epoch: 10  # 10 에폭부터 self-forcing 시작
  
  # Teacher-Student parameters
  use_teacher_student: true
  teacher_distill_weight: 1.0  # Distillation loss 가중치
  feature_distill_layers: [2, 3]  # layer_3, layer_4에서 feature distillation
  teacher_warmup_epochs: 0  # Teacher 워밍업 에폭 (선택적)
  # ---- 추가 Teacher-Student / Streaming 설정 ----
  cache_max_length: 32            # Temporal cache 최대 길이 (기존 32 하드코딩 -> 하이퍼파라미터화)
  distill_scale_invariant: true   # Distillation 시 scale/shift 정렬 적용 여부
  feature_distill_weight: 1.0     # Feature distillation loss 가중치
  depth_loss_weight: 1.0          # (신규) Depth supervision loss 가중치 (프레임 분할 후 적용)
  # 조정안 적용: distill_weight↓, feature_distill_weight↑ (실제 값은 train.py에서 override 가능)

  # ---- 캐시/스케일 정렬 추가 옵션 ----
  p_cache_reset: 0.01             # 일정 확률로 temporal cache 초기화 (robustness)
  ema_alpha: 0.10                 # (미사용 가능) scale/shift EMA blending 계수
  scale_reg: 0.001                # (옵션) scale/shift 정규화 가중치

  # ---- 로깅 / 분석 관련 ----
  log_gradient_norm: true         # gradient norm 에폭 평균 로깅
  log_scale_shift_stats: true     # a*, b* (scale, shift) 통계 로깅 (평균/표준편차)
  log_max_batches_per_epoch: null # (선택) 지정 시, train 로깅/학습을 앞 부분 일부 배치만 사용 (디버그)