import os
import random
from PIL import Image
import torch
import numpy as np
from torch.utils.data import Dataset
import torchvision.transforms.functional as TF
from glob import glob
import OpenEXR
import Imath
import matplotlib.pyplot as plt
import json


IMG_EXTS   = {'.png', '.jpg', '.jpeg', '.bmp'}
DEPTH_EXTS = {'.png', '.npy', '.exr', '.pfm'}  # í”„ë¡œì íŠ¸ì— ë§ê²Œ í•„ìš”ì‹œ ìˆ˜ì •
def _is_hidden_or_ckpt(path: str) -> bool:
    base = os.path.basename(path)
    return (base.startswith('.')) or ('.ipynb_checkpoints' in path)

def _sorted_files(dir_path: str, allowed_exts: set) -> list:
    """dir_path ì•ˆì—ì„œ í—ˆìš© í™•ì¥ìë§Œ, íŒŒì¼ë§Œ, ìˆ¨ê¹€/ì²´í¬í¬ì¸íŠ¸ ì œì™¸í•˜ì—¬ ì •ë ¬ ë°˜í™˜."""
    if not os.path.isdir(dir_path):
        return []
    try:
        entries = []
        for e in os.scandir(dir_path):
            if not e.is_file():
                continue
            if _is_hidden_or_ckpt(e.path):
                continue
            ext = os.path.splitext(e.name)[1].lower()
            if ext in allowed_exts:
                entries.append(e.path)
        # ìì—°ìŠ¤ëŸ¬ìš´ ì •ë ¬ (_natural_keyê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë ¬)
        try:
            entries.sort(key=_natural_key)  # dataLoader.pyì— ì´ë¯¸ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ
        except Exception:
            entries.sort()
        return entries
    except FileNotFoundError:
        return []

def to_tensor_safe_from_numpy(arr: np.ndarray, dtype=np.float32):
    """
    numpy â†’ torch í…ì„œ ë³€í™˜ ì‹œ, ìŒìˆ˜ stride/ë¹„ì—°ì†/ì½ê¸°ì „ìš© ë“±ì„
    np.ascontiguousarray + .contiguous().clone()ìœ¼ë¡œ ì•ˆì „í™”.
    """
    arr = np.asarray(arr, dtype=dtype)
    arr = np.ascontiguousarray(arr)      # ì—°ì† ë©”ëª¨ë¦¬ ë³´ì¥
    t = torch.from_numpy(arr).contiguous()
    return t.clone()                     # ìƒˆ storage í™•ë³´

def get_random_crop_params_with_rng(img, output_size, rng):
    w, h = img.size
    th, tw = output_size, output_size
    if w == tw and h == th:
        return 0, 0, th, tw
    i = rng.randint(0, h - th)
    j = rng.randint(0, w - tw)
    return i, j, th, tw


def get_data_list(root_dir, data_name, split, clip_len=16, condition_num=10, difficulties=None):
    """
    data pathë‘ clip_len, ì›í•˜ëŠ” ë°ì´í„° ê°œìˆ˜ or idx ê°œìˆ˜ë§Œí¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìŒ“ì•„ì£¼ëŠ” í•¨ìˆ˜
    """
    if data_name == "kitti":
        if split == "train":
            video_info_train = get_kitti_video_path(root_dir, condition_num=condition_num, split="train", binocular=False)
            x_path, y_path = get_kitti_individuals(video_info_train, clip_len, split) 
            return x_path, y_path
        else :
            video_info_val = get_kitti_video_path(root_dir, condition_num=condition_num, split="val", binocular=False)
            x_path, y_path, cam_ids, intrin_clips, extrin_clips = get_kitti_individuals(video_info_val, clip_len, split)
            return x_path, y_path, cam_ids, intrin_clips, extrin_clips

    elif data_name == "google":
        x_path, y_path = get_google_paths(root_dir)
        return x_path, y_path

    elif data_name == "GTA":
        if split == "train":
            x_path, y_path, _= get_GTA_paths(root_dir,split="train")
            return x_path, y_path
        else:
            x_path, y_path, poses_path = get_GTA_paths(root_dir,split="val")
            return x_path, y_path, poses_path 

            
    elif data_name == "tartanair":
        envs = split if isinstance(split, list) else [split]
        diff_list = difficulties if difficulties is not None else ['easy','hard']
        cams = ['lcam_front','lcam_right','lcam_back',
                'lcam_left','lcam_top','lcam_bottom']
        
        img_lists, dep_lists, pose_lists = get_tartanair_paths(
            root_dir, envs, diff_list, cams
        )
        # hardëŠ” í•™ìŠµì´ë¯€ë¡œ poseëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , easyëŠ” ê²€ì¦ì´ë¯€ë¡œ pose í¬í•¨ ë°˜í™˜
        if set(diff_list) == {'hard'}:
            return img_lists, dep_lists
        else:
            return img_lists, dep_lists, pose_lists

def get_tartanair_paths(root_dir, envs, difficulties, cams):
    """
    Returns three lists of lists: scene_img_lists, scene_dep_lists, scene_pose_lists.
    """
    scene_img_lists = []
    scene_dep_lists = []
    scene_pose_lists = []
    
    for env in envs:
        for diff in difficulties:
            for dirpath, dirnames, filenames in os.walk(root_dir):
                # normalize path separators
                parts = dirpath.replace('\\','/').split('/')
                # find env folder in path
                if env not in parts:
                    continue
                idx = parts.index(env)
                # must have Data_<diff> and Pxxx and image/depth folder after env
                if len(parts) <= idx + 3:
                    continue
                data_dir = parts[idx+1]
                chunk    = parts[idx+2]
                folder   = parts[idx+3]
                if data_dir != f"Data_{diff}" or not chunk.startswith('P'):
                    continue
                # build full chunk directory
                chunk_dir = os.path.join(*parts[:idx+3+1])  # reconstruct path to Pxxx
                # but safer to use dirpath
                chunk_dir = dirpath.rsplit(f"/{folder}",1)[0]
                # handle each camera
                for cam in cams:
                    if folder == f"image_{cam}":
                        img_dir = dirpath
                        dep_dir = img_dir.replace(f"image_{cam}", f"depth_{cam}")
                        pf_file = os.path.join(chunk_dir, f"pose_{cam}.txt")
                        if os.path.isdir(img_dir) and os.path.isdir(dep_dir) and os.path.isfile(pf_file):
                            imgs = sorted([os.path.join(img_dir,f) for f in os.listdir(img_dir)
                                           if f.lower().endswith(('.png','.jpg','.jpeg'))])
                            deps = sorted([os.path.join(dep_dir,f) for f in os.listdir(dep_dir)
                                           if f.lower().endswith('.png')])
                            if len(imgs) >= 1 and len(deps) >= 1:
                                scene_img_lists.append(imgs)
                                scene_dep_lists.append(deps)
                                scene_pose_lists.append(pf_file)

    if difficulties == ["hard"]:
        max_clips_per_scene = 1500
    else :
        max_clips_per_scene = 500

    # ì¼ë‹¨ì€ ì•„ë˜ì²˜ëŸ¼ í—ˆëŠ”ë°, ì•„ë˜ì²˜ëŸ¼ í•˜ë©´ í›„ì— ë°ì´í„° ì¶”ê°€ì‹œ, env ëŠ˜ì–´ë‚˜ë©´ ìˆ˜ì •í•´ì•¼í•  ìˆ˜ ìˆìŒ. ì¸ì§€í•˜ê¸°

    scene_img_lists  = [imgs[:max_clips_per_scene]  for imgs in scene_img_lists]
    scene_dep_lists  = [deps[:max_clips_per_scene]  for deps in scene_dep_lists]
    scene_pose_lists = scene_pose_lists.copy()

    max_total_clips = 90
    clip_len = 32
    # 3) ì „ì²´ í´ë¦½ ìˆ˜ ì œí•œ
    if max_total_clips is not None:
        accumulated = 0
        new_imgs, new_deps, new_poses = [], [], []
        for imgs, deps, pose in zip(scene_img_lists, scene_dep_lists, scene_pose_lists):
            # ì´ Sceneì´ ë§Œë“¤ ìˆ˜ ìˆëŠ” í´ë¦½ ìˆ˜
            available_clips = len(imgs) // clip_len
            if available_clips == 0:
                continue
            # ë‚¨ì€ í´ë¦½ ìˆ˜
            remaining = max_total_clips - accumulated
            if remaining <= 0:
                break

            # ì´ Sceneì—ì„œ ë½‘ì„ í´ë¦½ ìˆ˜
            take_clips = min(available_clips, remaining)
            take_frames = take_clips * clip_len  # í”„ë ˆì„ ìˆ˜ë¡œ í™˜ì‚°

            new_imgs.append(imgs[:take_frames])
            new_deps.append(deps[:take_frames])
            new_poses.append(pose)

            accumulated += take_clips

        scene_img_lists, scene_dep_lists, scene_pose_lists = new_imgs, new_deps, new_poses
            
    return scene_img_lists, scene_dep_lists, scene_pose_lists


def quaternion_to_rotmat(q):
    q = q / np.linalg.norm(q)
    qx, qy, qz, qw = q
    R = np.array([
        [1-2*(qy*qy+qz*qz), 2*(qx*qy - qz*qw), 2*(qx*qz + qy*qw)],
        [2*(qx*qy + qz*qw), 1-2*(qx*qx+qz*qz), 2*(qy*qz - qx*qw)],
        [2*(qx*qz - qy*qw), 2*(qy*qz + qx*qw), 1-2*(qx*qx+qy*qy)]
    ], dtype=np.float32)
    return R

def _natural_key(s):
    return [t.zfill(10) if t.isdigit() else t.lower() for t in re.findall(r'\d+|\D+', s)]

# sliding ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •í•˜ê¸°
def get_GTA_paths(root_dir, split):
    """
    return:
      all_images: List[List[str]]
      all_depths: List[List[str]]
      all_poses : List[List[str]]
    ê° ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ëŠ” ê°™ì€ scene ë‚´ í”„ë ˆì„ ìˆœì„œ.
    """
    all_depths, all_images, all_poses = [], [], []

    # scene ë””ë ‰í„°ë¦¬ë“¤ ì •ë ¬
    try:
        scenes = [d for d in os.listdir(root_dir) if not _is_hidden_or_ckpt(d)]
        try:
            scenes.sort(key=_natural_key)
        except Exception:
            scenes.sort()
    except FileNotFoundError:
        return all_images, all_depths, all_poses

    for scene in scenes:
        scene_dir   = os.path.join(root_dir, scene)
        depths_dir  = os.path.join(scene_dir, "depths")
        images_dir  = os.path.join(scene_dir, "images")
        poses_dir   = os.path.join(scene_dir, "poses")

        scene_depths = _sorted_files(depths_dir, DEPTH_EXTS)
        scene_images = _sorted_files(images_dir, IMG_EXTS)
        # posesëŠ” í…ìŠ¤íŠ¸/ë„˜íŒŒì´ ë“± í¬ë§·ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ í™•ì¥ì ì œí•œ ìµœì†Œí™”(íŒŒì¼ë§Œ & ìˆ¨ê¹€ ì œì™¸)
        scene_poses  = []
        if os.path.isdir(poses_dir):
            for e in os.scandir(poses_dir):
                if e.is_file() and not _is_hidden_or_ckpt(e.path):
                    scene_poses.append(e.path)
            try:
                scene_poses.sort(key=_natural_key)
            except Exception:
                scene_poses.sort()

        # ê¸¸ì´ ë¶ˆì¼ì¹˜ ì‹œ ìµœì†Œ ê¸¸ì´ì— ë§ì¶¤
        m = min(len(scene_images), len(scene_depths))
        if m == 0:
            # í•œìª½ì´ë¼ë„ ë¹„ì–´ ìˆìœ¼ë©´ í•´ë‹¹ sceneì€ ìŠ¤í‚µ
            continue

        scene_images = scene_images[:m]
        scene_depths = scene_depths[:m]
        scene_poses  = scene_poses[:m] if len(scene_poses) >= m else scene_poses

        all_depths.append(scene_depths)
        all_images.append(scene_images)
        all_poses.append(scene_poses)

    return all_images, all_depths, all_poses

def get_google_paths(root_dir):

    exts = ['.png', '.jpg', '.npy']
    x_paths = []
    y_paths = []
    
    img_root = os.path.join(root_dir, "images")
    dep_root = os.path.join(root_dir, "depth")

    for folder in sorted(os.listdir(img_root)):
        img_dir = os.path.join(img_root, folder)
        dep_dir = os.path.join(dep_root, folder)
        if not os.path.isdir(img_dir):
            print(f"ê²½ê³ : ì´ë¯¸ì§€ í´ë” ì—†ìŒ: {img_dir}")
            continue
        if not os.path.isdir(dep_dir):
            print(f"ê²½ê³ : depth í´ë” ì—†ìŒ: {dep_dir}")
            continue

        for fname in sorted(os.listdir(img_dir)):
            img_path = os.path.join(img_dir, fname)
            base, _ = os.path.splitext(fname)
            for ext in exts:
                dep_path = os.path.join(dep_dir, base + ext)
                if os.path.isfile(dep_path):
                    x_paths.append(img_path)
                    y_paths.append(dep_path)
                    break
            else:
                print(f"ê²½ê³ : ëŒ€ì‘í•˜ëŠ” depth íŒŒì¼ ì—†ìŒ: {img_path}")

    return x_paths, y_paths

def get_kitti_individuals(video_info, clip_len, split):
    x_clips, y_clips, intrin_clips, extrin_clips, cam_ids = [], [], [], [], []

    for info in video_info:
        rgb_dir        = info['rgb_path']
        depth_dir      = info['depth_path']
        intrinsic_file = info['intrinsic_file']
        extrinsic_file = info['extrinsic_file']
        camera_id      = info['camera']

        rgb_files   = sorted(os.listdir(rgb_dir))
        depth_files = sorted(os.listdir(depth_dir))
        if len(rgb_files) != len(depth_files):
            continue

        # ğŸ”‘ ë³€ê²½: sliding window
        n = len(rgb_files) - clip_len + 1
        if n <= 0:
            continue

        x_clips.append([os.path.join(rgb_dir, f) for f in rgb_files])
        y_clips.append([os.path.join(depth_dir, f) for f in depth_files])
        intrin_clips.append(intrinsic_file)
        extrin_clips.append(extrinsic_file)
        cam_ids.append(camera_id)

    if split == "train":
        return x_clips, y_clips
    else:
        return x_clips, y_clips, cam_ids, intrin_clips, extrin_clips
    
    
    
def get_kitti_video_path(root_dir, condition_num, split, binocular):
    """
    condition_num: ê° sceneì—ì„œ ëª‡ ê°œì˜ conditionì„ ê°€ì ¸ì˜¬ì§€
    split: "train" ë˜ëŠ” "val" (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    binocular: Trueë©´ Camera_0, Camera_1 ëª¨ë‘ / Falseë©´ Camera_0ë§Œ
    """
    rgb_root    = os.path.join(root_dir, "vkitti_2.0.3_rgb")
    depth_root  = os.path.join(root_dir, "vkitti_2.0.3_depth")
    textgt_root = os.path.join(root_dir, "vkitti_2.0.3_textgt")

    video_infos = []

    # scene ë””ë ‰í„°ë¦¬ ìˆœíšŒ
    for scene in sorted(os.listdir(rgb_root)):
        if _is_hidden_or_ckpt(scene):
            continue

        scene_rgb_path    = os.path.join(rgb_root, scene)
        scene_depth_path  = os.path.join(depth_root, scene)
        scene_textgt_path = os.path.join(textgt_root, scene)

        if not (os.path.isdir(scene_rgb_path) and os.path.isdir(scene_depth_path) and os.path.isdir(scene_textgt_path)):
            continue

        # ê¸°ì¡´ split ê¸°ì¤€ ìœ ì§€
        if (split == "train" and "Scene06" in scene) or (split == "val" and "Scene06" not in scene):
            continue

        # condition ìˆœíšŒ
        picked = 0
        for condition in sorted(os.listdir(scene_rgb_path)):
            if _is_hidden_or_ckpt(condition):
                continue

            cond_rgb_path    = os.path.join(scene_rgb_path, condition)
            cond_depth_path  = os.path.join(scene_depth_path, condition)
            cond_textgt_path = os.path.join(scene_textgt_path, condition)
            if not (os.path.isdir(cond_rgb_path) and os.path.isdir(cond_depth_path) and os.path.isdir(cond_textgt_path)):
                continue

            intrinsic_file = os.path.join(cond_textgt_path, "intrinsic.txt")
            extrinsic_file = os.path.join(cond_textgt_path, "extrinsic.txt")
            if not (os.path.isfile(intrinsic_file) and os.path.isfile(extrinsic_file)):
                # print(f"ê²½ê³ : {cond_textgt_path}ì— intrinsic/extrinsic ëˆ„ë½")
                continue

            cams = ["Camera_0", "Camera_1"] if binocular else ["Camera_0"]
            for cam in cams:
                cam_idx   = int(cam[-1])
                rgb_path  = os.path.join(cond_rgb_path,   "frames", "rgb",   cam)
                depth_path= os.path.join(cond_depth_path, "frames", "depth", cam)

                # í´ë” ì¡´ì¬/íŒŒì¼ ì¡´ì¬ ìµœì†Œ í™•ì¸
                rgb_files   = _sorted_files(rgb_path,   IMG_EXTS)
                depth_files = _sorted_files(depth_path, DEPTH_EXTS)

                if (len(rgb_files) == 0) or (len(depth_files) == 0):
                    # ì´ë¯¸ì§€ê°€ ì‹¤ì œë¡œ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë””ë ‰í„°ë¦¬ë§Œ ì¡´ì¬í•˜ëŠ” ì¼€ì´ìŠ¤ ë°©ì§€)
                    continue

                video_infos.append({
                    'rgb_path': rgb_path,
                    'depth_path': depth_path,
                    'intrinsic_file': intrinsic_file,
                    'extrinsic_file': extrinsic_file,
                    'scene': scene,
                    'condition': condition,
                    'camera': cam_idx
                })

            picked += 1
            if picked >= max(1, int(condition_num)):
                break

    return video_infos

class KITTIVideoDataset(Dataset):
    def __init__(
        self,
        rgb_paths,
        depth_paths,
        cam_ids=None,
        intrin_clips=None,
        extrin_clips=None,
        seed = 42,
        rgb_mean=(0.485, 0.456, 0.406),
        rgb_std=(0.229, 0.224, 0.225),
        resize_size=350,
        split="train",
        clip_len=32,
    ):
        super().__init__()
        assert split in ["train", "val"]
        assert len(rgb_paths) == len(depth_paths)
        self.rgb_paths = rgb_paths
        self.depth_paths = depth_paths
        self.intrin_clips  = intrin_clips
        self.extrin_clips  = extrin_clips
        self.cam_ids = cam_ids
        self.rgb_mean = rgb_mean
        self.rgb_std = rgb_std
        self.resize_size = resize_size
        self.split = split
        self.seed = seed
        self.epoch = 0
        self.clip_len = clip_len

        # sceneë³„ë¡œ  effective clip ê³„ì‚°
        scene_clip_counts = [
            max(0, len(scene_rgb) - self.clip_len + 1)
            for scene_rgb in self.rgb_paths
        ]

        # ì´ í´ë¦½ ê°œìˆ˜
        self.total_clips = sum(scene_clip_counts)

        # flat idx -> scene_idx, chunk_idx
        self.flat2scene = [0] * self.total_clips
        self.flat2chunk = [0] * self.total_clips

        ptr = 0
        for scene_idx, n_eff in enumerate(scene_clip_counts):
            for chunk_idx in range(n_eff):
                self.flat2scene[ptr] = scene_idx
                self.flat2chunk[ptr] = chunk_idx
                ptr += 1
        
        if split == "train" :
            print("train_VKITTI_total_clips : ",self.total_clips)
        else :
            print("val_VKITTI_total_clips : ",self.total_clips)
        
    def set_epoch(self, epoch):
        self.epoch = epoch

    def __len__(self):
        return self.total_clips

    def load_depth(self, path):
        depth_png = Image.open(path)
        depth_cm = np.array(depth_png, dtype=np.uint16).astype(np.float32)
        depth_m = depth_cm / 100.0
        depth_img = Image.fromarray ((depth_m), mode="F")

        return depth_img

    @staticmethod
    def load_camera_params(intrinsic_path, extrinsic_path):
        """
        intrinsic.txt, extrinsic.txt íŒŒì¼ì„ ì½ì–´ ë‘ ê°œì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        - intrinsics: {(frame, camera_id): [fx, fy, cx, cy]}
        - extrinsics: {(frame, camera_id): 4x4 í–‰ë ¬}
        """
        intrinsics = {}
        with open(intrinsic_path, 'r') as f:
            lines = f.readlines()
            for line in lines[1:]:
                parts = line.strip().split()
                if len(parts) < 6:
                    continue
                frame = int(parts[0])
                camera_id = int(parts[1])
                fx = float(parts[2])
                fy = float(parts[3])
                cx = float(parts[4])
                cy = float(parts[5])
                intrinsics[(frame, camera_id)] = [fx, fy, cx, cy]

        extrinsics = {}
        with open(extrinsic_path, 'r') as f:
            lines = f.readlines()
            for line in lines[1:]:
                parts = line.strip().split()
                if len(parts) < 18:
                    continue
                frame = int(parts[0])
                camera_id = int(parts[1])
                matrix_vals = list(map(float, parts[2:18]))
                transform = np.array(matrix_vals).reshape((4, 4))
                extrinsics[(frame, camera_id)] = transform

        return intrinsics, extrinsics

    @staticmethod
    def get_camera_parameters(frame, camera_id, intrinsics, extrinsics):
        """
        (frame, camera_id)ì— í•´ë‹¹í•˜ëŠ” ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        intrinsic_params = intrinsics.get((frame, camera_id))
        extrinsic_matrix = extrinsics.get((frame, camera_id))
        return intrinsic_params, extrinsic_matrix

    @staticmethod
    def get_projection_matrix(frame, camera_id, intrinsics, extrinsics):
        """
        (frame, camera_id)ì— í•´ë‹¹í•˜ëŠ” 3x4 íˆ¬ì˜ í–‰ë ¬ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        intrinsic_params, extrinsic_matrix = KITTIVideoDataset.get_camera_parameters(
            frame, camera_id, intrinsics, extrinsics
        )
        if intrinsic_params is None or extrinsic_matrix is None:
            return None

        fx, fy, cx, cy = intrinsic_params
        K = np.array([[fx, 0, cx],
                      [0, fy, cy],
                      [0, 0, 1]])
        RT = extrinsic_matrix[:3, :]
        P = K @ RT  # 3x4 íˆ¬ì˜í–‰ë ¬
        return P
    
    
    def __getitem__(self, idx):
        """
        ì§€ê¸ˆ ì—¬ê¸°ì— ë“¤ì–´ì˜¨ rgb_pathsë“¤ì€ ê·¸ëƒ¥ ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŒ  -> ì´ì¤‘ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì§€ê³ ìˆìŒ sceneë³„ë¡œ
        end_idxë¡œ ì´ê±¸ ì ì ˆíˆ í•¸ë“¤ë§ í•´ì•¼í•¨
        í˜„ì¬ ë°›ì•„ì˜¤ëŠ” idxëŠ” ì „ì²´ ì˜ìƒê¸¸ì´ / clip_lenìœ¼ë¡œ í• ê±°ì„ -> ì•„ ì´ëŸ¬ë©´ ì•ˆë˜ëŠ”ê²Œ, sceneë³„ë¡œ ë‚˜ëˆ ì§€ëŠ” ëª« ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼í•¨ ã…‡ã…‡ ê·¸ì¹˜
        ë¬´íŠ¼ ì¼ë‹¨ idxëŠ” ê° clipì˜ ê°œìˆ˜ë¼ê³  ìƒê°í•´ë³´ì.

        ì •ë¦¬í•´ë³´ìë©´, idxëŠ” clip idxë¥¼ ë„£ì–´ì•¼í•¨. ì¦‰ 
        """

        ## algorithm : ê²°êµ­ start indexë§Œ ì˜ ë½‘ìœ¼ë©´ í•´ê²°ë˜ëŠ” ë¬¸ì œì„
        ## ì£¼ì˜í•´ì•¼í•  ì ì€, ìœ„ì—ì„œ ë„˜ê²¨ì¤„ë•Œ 16ìœ¼ë¡œ ë‚˜ëˆ  ë–¨ì–´ì§€ëŠ”ê±°ë§Œ ì¤€ê²Œ ì•„ë‹˜.-> ì´ëŸ¬ë©´ ë¬¸ì œê°€, ë‚˜ë¨¸ì§€ê°€ ê°ê° ë‹¤ë¥´ë‹ˆê¹Œ ê·¸ëƒ¥ clip lenë§Œí¼ìœ¼ë¡œ ìë¥´ì. ìˆ˜ì •í–ˆìŒ
        ## shiftëŠ” 0ë¶€í„° 15ê¹Œì§€ ê°€ëŠ¥.
        ## 64ê°œ ì˜€ë‹¤ë©´, -> ì´ê±¸ ë‚˜ëˆ„ê¸° 16 í•˜ë©´ 4ê°œê°€ ë‚˜ì˜¤ëŠ”ë°
        """
        remaining = idx ## ì˜ˆë¥¼ ë“¤ì–´ idxê°€ 5ì´ë‹¤. ì¦‰ 5ë²ˆì§¸ í´ë¦½ì„ ë°›ëŠ” íƒ€ì´ë°ì´ë¼ê³  í•´ë³´ì
        for scene_idx, scene_rgb in enumerate(self.rgb_paths):
            n_clips = len(scene_rgb)// self.clip_len    # ë§Œì•½ 1ë²ˆì§¸ ì”¬ì´ 64ê°œë¼ê³  í•´ë³´ë©´, nclip = 4
            effective = n_clips - 1 # ë§ˆì§€ë§‰êº¼ ë²„ë¦¼ ì´ìŠˆ for overflow ë°©ì§€
            if remaining < effective:
                break
            remaining -= effective    # remaining = 2
        chunk_idx = remaining   # forë¬¸ì„ ë‚˜ì˜¤ê³  ë‚˜ë©´, chunk idxëŠ” scene_idxì— í•´ë‹¹í•˜ëŠ” ì”¬ì˜ ëª‡ë²ˆì§¸ í´ë¦½ì¸ì§€ê°€ ë¨

        -> ì´ê±° ì˜¤ë²„í—¤ë“œ ë„ˆë¬´í¼ ìƒê°í•´ë³´ë©´. ìœ„ë¡œ ì˜¬ë¦¬ê¸°
        """

                # 2) train splitì´ë©´ ì—¬ê¸°ì„œ ë°”ë¡œ ë°˜í™˜
        if self.split == "train":

            scene_idx = self.flat2scene[idx]
            chunk_idx = self.flat2chunk[idx]

            scene_rgb_paths   = self.rgb_paths[scene_idx]
            scene_depth_paths = self.depth_paths[scene_idx]

            rng = random.Random(self.seed + self.epoch)
            shift = rng.randint(0, self.clip_len-1)

            base = chunk_idx + shift
            rgb_paths  = scene_rgb_paths[base: base+self.clip_len]
            depth_paths= scene_depth_paths[base: base+self.clip_len]

            #rgb_paths = self.rgb_clips[idx]
            #depth_paths = self.depth_clips[idx]

            first = Image.open(rgb_paths[0]).convert("RGB")
            first = TF.resize(first, self.resize_size)
            i, j, th, tw = get_random_crop_params_with_rng(first, self.resize_size, rng)
            
            rgb_seq, depth_seq = [], []
            for rp, dp in zip(rgb_paths, depth_paths):
                img = Image.open(rp).convert("RGB")
                img = TF.resize(img, self.resize_size)
                img = TF.crop(img, i, j, th, tw)
                img = TF.normalize(TF.to_tensor(img), mean=self.rgb_mean, std=self.rgb_std)
                rgb_seq.append(img.contiguous().clone())

                depth = self.load_depth(dp)
                depth = TF.resize(depth, self.resize_size, antialias=True)
                depth = TF.crop(depth, i, j, th, tw)
                depth_seq.append(TF.to_tensor(depth))

            rgb_tensor = torch.stack(rgb_seq)     # [clip_len, 3, H, W]
            depth_tensor = torch.stack(depth_seq) # [clip_len, 1, H, W]

            return rgb_tensor, depth_tensor


        else :

            scene_idx = self.flat2scene[idx]
            chunk_idx = self.flat2chunk[idx]
            
            scene_rgb_paths   = self.rgb_paths[scene_idx]
            scene_depth_paths = self.depth_paths[scene_idx]

            base = chunk_idx * self.clip_len
            rgb_paths  = scene_rgb_paths  [base:base+self.clip_len]
            depth_paths= scene_depth_paths[base:base+self.clip_len]

            rgb_seq, depth_seq = [], []
            for rp, dp in zip(rgb_paths, depth_paths):
                img = Image.open(rp).convert("RGB")
                img = TF.resize(img, self.resize_size)
                img = TF.center_crop(img, self.resize_size)
                img = TF.normalize(TF.to_tensor(img), mean=self.rgb_mean, std=self.rgb_std)
                rgb_seq.append(img.contiguous().clone())

                depth = self.load_depth(dp)
                depth = TF.resize(depth, self.resize_size, antialias=True)
                depth = TF.center_crop(depth, self.resize_size)
                depth_seq.append(TF.to_tensor(depth))

            rgb_tensor = torch.stack(rgb_seq)     # [clip_len, 3, H, W]
            depth_tensor = torch.stack(depth_seq) # [clip_len, 1, H, W]

            
            # 3) val splitì¼ ë•Œë§Œ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ë¡œë”©
            camera_id = self.cam_ids[scene_idx]
            intrinsic_file = self.intrin_clips[scene_idx]
            extrinsic_file = self.extrin_clips[scene_idx]
            intrinsics_dict, extrinsics_dict = self.load_camera_params(intrinsic_file, extrinsic_file)

            extrinsics_list, intrinsics_list = [], []
            for dp in depth_paths:
                frame_num = int(os.path.splitext(os.path.basename(dp))[0].split('_')[-1])
                intr_p, extr_m = self.get_camera_parameters(frame_num, camera_id, intrinsics_dict, extrinsics_dict)

                if extr_m is None:
                    extr_m = np.eye(4, dtype=np.float32)
                extrinsics_list.append(torch.tensor(extr_m, dtype=torch.float32))

                if intr_p is None:
                    fx, fy, cx, cy = 725.0087, 725.0087, 620.5, 187.0
                else:
                    fx, fy, cx, cy = intr_p
                K = torch.tensor([[fx, 0.0, cx],
                                [0.0, fy, cy],
                                [0.0, 0.0, 1.0]], dtype=torch.float32)
                intrinsics_list.append(K)

            extrinsics_tensor = torch.stack(extrinsics_list)   # [clip_len, 4, 4]
            intrinsics_tensor = torch.stack(intrinsics_list)   # [clip_len, 3, 3]
            return rgb_tensor, depth_tensor, extrinsics_tensor, intrinsics_tensor


class GTADataset(Dataset):
    def __init__(
        self,
        rgb_paths,
        depth_paths,
        pose_paths=None,
        split="train",
        clip_len=32,
        resize_size=350,
        rgb_mean=(0.485, 0.456, 0.406),
        rgb_std=(0.229, 0.224, 0.225),
        seed=42,
        stride=1,          # â˜… ì¶”ê°€: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìŠ¤íŠ¸ë¼ì´ë“œ (ê¸°ë³¸ 1)
        jitter=1,        # (ì„ íƒ) ì‹œì‘ì ì— Â±jitter ëœë¤ ë³€ë™ì„ ì£¼ê³  ì‹¶ë‹¤ë©´
    ):
        super().__init__()
        assert split in ["train", "val"]
        self.rgb_paths = rgb_paths
        self.depth_paths = depth_paths
        self.pose_paths = pose_paths
        self.clip_len = clip_len
        self.resize_size = resize_size
        self.rgb_mean = rgb_mean
        self.rgb_std = rgb_std
        self.seed = seed
        self.split = split
        self.epoch = 0
        self.stride = stride
        self.jitter = jitter
        
        # --- ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‹œì‘ì  ê³„ì‚° ---
        self.starts_per_scene = []
        total = 0
        for scene_rgb in self.rgb_paths:
            n = len(scene_rgb)
            if n < clip_len:
                self.starts_per_scene.append([])  # ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì”¬
                continue
            starts = list(range(0, n - clip_len + 1, self.stride))  # â˜… í•µì‹¬: 0..(N-L)
            self.starts_per_scene.append(starts)
            total += len(starts)

        self.total_clips = total

        # flat index ë§¤í•‘
        self.flat2scene = []
        self.flat2start = []
        for scene_idx, starts in enumerate(self.starts_per_scene):
            for s in starts:
                self.flat2scene.append(scene_idx)
                self.flat2start.append(s)

        if split == "train":
            print("train_GTA_total_clips :", self.total_clips)
        else:
            print("val_GTA_total_clips :", self.total_clips)

    def __len__(self):
        return self.total_clips

    def set_epoch(self, epoch):
        self.epoch = epoch

    def load_depth(self, path):
        exr_file = OpenEXR.InputFile(path)
        header   = exr_file.header()
        dw       = header['dataWindow']
        width    = dw.max.x - dw.min.x + 1
        height   = dw.max.y - dw.min.y + 1

        pt       = Imath.PixelType(Imath.PixelType.FLOAT)
        raw_str  = exr_file.channel('Y', pt)
        depth_np = np.frombuffer(raw_str, dtype=np.float32).reshape((height, width))
        
        # âœ… inf/nan ê°’ ì²˜ë¦¬ - ë§¤ìš° í° ê°’(1000m)ìœ¼ë¡œ ì œí•œ
        depth_np = np.nan_to_num(depth_np, nan=0.0, posinf=1000.0, neginf=0.0)

        # âœ… ì—°ì† ë©”ëª¨ë¦¬ + ìƒˆ storage ë³´ì¥
        depth_t  = to_tensor_safe_from_numpy(depth_np, dtype=np.float32)  # [H,W], contiguous+clone
        return depth_t.unsqueeze(0)  # [1,H,W]

    def __getitem__(self, idx):
        scene_idx = self.flat2scene[idx]
        start     = self.flat2start[idx]    # â˜… ê³ ì • ì‹œì‘ì  (strideì— ì˜í•´ ê²°ì •)

        rgb_list   = self.rgb_paths[scene_idx]
        depth_list = self.depth_paths[scene_idx]
        end = start + self.clip_len

        # (ì„ íƒ) jitterë¥¼ ì£¼ê³  ì‹¶ë‹¤ë©´:
        if self.split == "train" and self.jitter > 0:
            rng = random.Random(self.seed + self.epoch + idx)
            delta = rng.randint(-self.jitter, self.jitter)
            start = max(0, min(start + delta, len(rgb_list) - self.clip_len))
            end = start + self.clip_len

        rgb_clip   = rgb_list[start:end]
        depth_clip = depth_list[start:end]

        # ì „ì²˜ë¦¬
        rng = random.Random(self.seed + self.epoch)
        first = Image.open(rgb_clip[0]).convert("RGB")
        first = TF.resize(first, self.resize_size, antialias=True)
        if self.split == "train":
            i, j, h, w = get_random_crop_params_with_rng(first, self.resize_size, rng)

        rgb_seq, depth_seq = [], []
        for rp, dp in zip(rgb_clip, depth_clip):
            img = Image.open(rp).convert("RGB")
            img = TF.resize(img, self.resize_size, antialias=True)
            img = TF.crop(img, i, j, h, w) if self.split == "train" else TF.center_crop(img, self.resize_size)
            img = TF.normalize(TF.to_tensor(img), mean=self.rgb_mean, std=self.rgb_std)
            rgb_seq.append(img.contiguous().clone())

            dimg = self.load_depth(dp)                                  # [1,H,W] torch
            dimg = TF.resize(dimg, self.resize_size, antialias=True)
            dimg = TF.crop(dimg, i, j, h, w) if self.split == "train" else TF.center_crop(dimg, self.resize_size)
            depth_seq.append(dimg.contiguous().clone())

        rgb_tensor   = torch.stack(rgb_seq)   # [clip_len, 3, H, W]
        depth_tensor = torch.stack(depth_seq) # [clip_len, 1, H, W]

        if self.split == "train":
            return rgb_tensor, depth_tensor

        # val: load camera params from JSON pose files
        intrinsics_list, extrinsics_list = [], []
        for pp in pose_clip:
            with open(pp, 'r') as f:
                data = json.load(f)
            fx, fy = data['f_x'], data['f_y']
            cx, cy = data['c_x'], data['c_y']
            K = torch.tensor([[fx, 0.0, cx],
                              [0.0, fy, cy],
                              [0.0, 0.0, 1.0]], dtype=torch.float32)
            intrinsics_list.append(K)

            ext = torch.tensor(data['extrinsic'], dtype=torch.float32)
            extrinsics_list.append(ext)

        intrinsics_tensor = torch.stack(intrinsics_list)   # [clip_len, 3, 3]
        extrinsics_tensor = torch.stack(extrinsics_list)   # [clip_len, 4, 4]

        return rgb_tensor, depth_tensor, extrinsics_tensor, intrinsics_tensor



class GoogleDepthDataset(Dataset):
    def __init__(
        self,
        img_paths,
        depth_paths,
        rgb_mean=(0.485, 0.456, 0.406),
        rgb_std=(0.229, 0.224, 0.225),
        resize_size=518,
        seed=42
    ):
        super().__init__()
        assert len(img_paths) == len(depth_paths), "ì´ë¯¸ì§€/ëìŠ¤ ê°œìˆ˜ ë¶ˆì¼ì¹˜"
        self.img_paths   = img_paths
        self.depth_paths = depth_paths
        self.resize_size = resize_size
        self.rgb_mean = rgb_mean
        self.rgb_std = rgb_std
        self.seed = seed
        self.epoch = 0

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):

        rng = random.Random(self.seed + self.epoch)
        # RGB ì´ë¯¸ì§€ 

        first = Image.open(self.img_paths[0]).convert("RGB")
        first = TF.resize(first, self.resize_size)
        i, j, th, tw = get_random_crop_params_with_rng(first, self.resize_size, rng)

        img = Image.open(self.img_paths[idx]).convert("RGB")
        img = TF.resize(img, self.resize_size)
        img = TF.crop(img, i, j, th, tw)
        img = TF.normalize(TF.to_tensor(img),mean=self.rgb_mean,std=self.rgb_std)

        # Disparity/Depth
        dp = self.depth_paths[idx]
        if dp.endswith('.npy'):
            disp = np.load(dp).astype(np.float32)
            disp_img = Image.fromarray(disp)
        else:
            disp_img = Image.open(dp).convert("F")
        disp_img = TF.resize(disp_img, self.resize_size)
        disp_img = TF.crop(disp_img, i, j, th, tw)
        disp = torch.from_numpy(np.array(disp_img, np.float32)).unsqueeze(0)

        return img,disp

class CombinedDataset(Dataset):
    def __init__(self, kitti_dataset, google_dataset,ratio=4):
        super().__init__()
        self.ratio = ratio
        self.kitti_dataset = kitti_dataset
        self.google_dataset = google_dataset
        
        #print("kitti len ",len(self.kitti_dataset))
        #print("google len ",len(self.google_dataset) )

    def set_epoch(self, epoch):
        self.kitti_dataset.set_epoch(epoch)
        
    def __len__(self):
        return min(len(self.kitti_dataset), len(self.google_dataset)// self.ratio)
    
    def __getitem__(self, idx):
        kitti_item = self.kitti_dataset[idx]
        start = idx * self.ratio
        google_items = [self.google_dataset[start + i] for i in range(self.ratio)]
        
        google_imgs  = torch.stack([item[0] for item in google_items], dim=0)  # [ratio, 3, H, W]
        google_depths = torch.stack([item[1] for item in google_items], dim=0)  # [ratio, 1, H, W]

        return kitti_item, (google_imgs,google_depths)


class TartanAirVideoDataset(Dataset):
    """
    Returns:
      train: (rgb[T,3,H,W], depth[T,1,H,W])
      val:   (rgb[T,3,H,W], depth[T,1,H,W], extrinsics[T,4,4], intrinsics[T,3,3])
    DepthëŠ” ì›ë³¸ ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    Supports optional depth range attributes but does not perform masking here.
    """
    def __init__(
        self,
        img_lists,
        dep_lists,
        posefile_lists=None,
        clip_len=16,
        resize_size=350,
        rgb_mean=(0.485,0.456,0.406),
        rgb_std=(0.229,0.224,0.225),
        split='train',
        seed=42,
    ):
        super().__init__()
        self.img_lists      = img_lists
        self.dep_lists      = dep_lists
        self.posefile_lists = posefile_lists or []
        self.clip_len       = clip_len
        self.resize_size    = resize_size
        self.rgb_mean       = rgb_mean
        self.rgb_std        = rgb_std
        self.split          = split
        self.seed           = seed
        self.epoch          = 0

        self.flat2scene, self.flat2chunk = [], []
        for si, imgs in enumerate(self.img_lists):
            n_chunks = len(imgs) // self.clip_len
            for ci in range(n_chunks):
                self.flat2scene.append(si)
                self.flat2chunk.append(ci)

        fx, fy, cx, cy = 320.0, 320.0, 320.0, 240.0
        K = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=np.float32)
        self.K = torch.from_numpy(K)

    def __len__(self):
        return len(self.flat2scene)

    def set_epoch(self, epoch):
        self.epoch = epoch

    def __getitem__(self, idx):
        si, ci = self.flat2scene[idx], self.flat2chunk[idx]
        base   = ci * self.clip_len

        clip_imgs = self.img_lists[si][base:base + self.clip_len]
        clip_deps = self.dep_lists[si][base:base + self.clip_len]

        rng = random.Random(self.seed + self.epoch)
        shift = rng.randint(0, self.clip_len - 1) if self.split == 'train' else 0

        first = Image.open(clip_imgs[0]).convert('RGB')
        first = TF.resize(first, self.resize_size)
        if self.split == 'train':
            i, j, h, w = get_random_crop_params_with_rng(first, self.resize_size, rng)
            crop = (i, j, h, w)
        else:
            crop = (0, 0, self.resize_size, self.resize_size)

        rgb_seq, depth_seq = [], []
        for img_p, dep_p in zip(clip_imgs, clip_deps):
            img = Image.open(img_p).convert('RGB')
            img = TF.resize(img, self.resize_size)
            if self.split == 'train':
                img = TF.crop(img, *crop)
            else:
                img = TF.center_crop(img, self.resize_size)
            img = TF.normalize(TF.to_tensor(img), mean=self.rgb_mean, std=self.rgb_std)
            rgb_seq.append(img)

            d_np = np.array(Image.open(dep_p).convert('F'), dtype=np.float32)
            d_t = torch.from_numpy(d_np).unsqueeze(0)  # [1,H,W]
            d_t = TF.resize(d_t, self.resize_size)
            if self.split == 'train':
                d_t = TF.crop(d_t, *crop)
            else:
                d_t = TF.center_crop(d_t, self.resize_size)
            depth_seq.append(d_t)

        rgb_t   = torch.stack(rgb_seq)   # [T,3,H,W]
        depth_t = torch.stack(depth_seq) # [T,1,H,W]

        if self.split == 'val':
            pf = self.posefile_lists[si]
            lines = open(pf, 'r').read().splitlines()
            Es, Ks = [], []
            for fidx in range(base + shift, base + shift + self.clip_len):
                vals = list(map(float, lines[fidx].split()))
                t_vec = np.array(vals[:3], dtype=np.float32)
                q     = np.array(vals[3:], dtype=np.float32)
                R     = quaternion_to_rotmat(q)
                E     = np.eye(4, dtype=np.float32)
                E[:3,:3], E[:3,3] = R, t_vec
                Es.append(torch.from_numpy(E))
                Ks.append(self.K)
            E_t = torch.stack(Es)  # [T,4,4]
            K_t = torch.stack(Ks)  # [T,3,3]
            return rgb_t, depth_t, E_t, K_t

        return rgb_t, depth_t


class CombinedDataset_NoSingleImg(Dataset):
    def __init__(self, kitti_dataset, gta_dataset):
        super().__init__()
        self.datasets = [kitti_dataset, gta_dataset, tartanair_dataset]
        self.lens = [len(ds) for ds in self.datasets]
        self.min_len = min(self.lens)
        # ì „ì²´ ê¸¸ì´ëŠ” 3 * min_len (ì„¸ ë°ì´í„°ì…‹ì„ ë²ˆê°ˆì•„ ìˆœíšŒ)
        self.total = len(self.datasets) * self.min_len

        print(f"kitti ë°ì´í„° ê°œìˆ˜: {self.lens[0]}")
        print(f"GTA ë°ì´í„° ê°œìˆ˜: {self.lens[1]}")
        print(f"TartanAir ë°ì´í„° ê°œìˆ˜: {self.lens[2]}")

    def __len__(self):
        return self.total

    def __getitem__(self, idx):
        ds_idx = idx % len(self.datasets)
        sample_idx = idx // len(self.datasets)
        return self.datasets[ds_idx][sample_idx]