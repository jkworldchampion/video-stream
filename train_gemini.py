import os
import argparse
import logging
import torch
import numpy as np
import yaml
import wandb
from dotenv import load_dotenv
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
from PIL import Image

# (ìˆ˜ì •) DDPë¥¼ ìœ„í•œ ëª¨ë“ˆ ì„í¬íŠ¸
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# í”„ë¡œì íŠ¸ ì˜ì¡´ì„±
from utils.loss_MiDas import *
from data.dataLoader import *
from data.val_dataLoader import *
from video_depth_anything.video_depth_stream import VideoDepthAnything
from benchmark.eval.metric import *
from benchmark.eval.eval_tae import tae_torch

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
experiment = 11

def setup_ddp():
    """DDPë¥¼ ìœ„í•œ ë¶„ì‚° í™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    dist.init_process_group(backend="nccl")
    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))

def cleanup_ddp():
    """DDP í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    dist.destroy_process_group()

def is_main_process():
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ê°€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤(rank 0)ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return dist.get_rank() == 0

def setup_logging(is_main):
    """ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ë¡œê·¸ì™€ íŒŒì¼ í•¸ë“¤ëŸ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    os.makedirs("logs", exist_ok=True)
    level = logging.INFO if is_main else logging.WARNING
    
    # (ìˆ˜ì •) íŒŒì¼ í•¸ë“¤ëŸ¬ëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ë§Œ ì¶”ê°€
    handlers = [logging.StreamHandler()]
    if is_main:
        handlers.append(logging.FileHandler(f"logs/train_log_experiment_{experiment}.txt"))

    logging.basicConfig(
        level=level,
        format="%(asctime)s RANK:%(process)d %(levelname)-8s %(message)s",
        handlers=handlers,
    )
    return logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸/í‰ê°€ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# least_square_whole_clip, eval_tae, metric_val, get_mask, norm_ssi ë“±
# í‰ê°€ ê´€ë ¨ í•¨ìˆ˜ë“¤ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.
# (ì´ ë¶€ë¶„ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ë¯€ë¡œ ê°„ê²°í•¨ì„ ìœ„í•´ ìƒëµ)
def least_square_whole_clip(infs, gts, data_name):
    # [B,T,1,H,W] â†’ [B,T,H,W]
    if infs.dim() == 5 and infs.shape[2] == 1:
        infs = infs.squeeze(2)
    if gts.dim() == 5 and gts.shape[2] == 1:
        gts = gts.squeeze(2)

    if data_name == "kitti":
        valid_mask = (gts > 1e-3) & (gts < 80.0)
    elif data_name == "gta":
        valid_mask = (gts > 1e-3) & (gts < 1000.0)
    elif data_name == "tartanair":
        valid_mask = (gts > 60.0) & (gts < 150.0)
    else:
        valid_mask = (gts > 1e-3) & (gts < 10.0)

    gt_disp_masked = 1.0 / (gts[valid_mask].reshape((-1, 1)).double() + 1e-6)
    infs = infs.clamp(min=1e-3)
    pred_disp_masked = infs[valid_mask].reshape((-1, 1)).double()

    A = torch.cat([pred_disp_masked, torch.ones_like(pred_disp_masked)], dim=-1)
    X = torch.linalg.lstsq(A, gt_disp_masked).solution
    scale, shift = X[0].item(), X[1].item()

    aligned_pred = torch.clamp(scale * infs + shift, min=1e-3)
    depth = torch.zeros_like(aligned_pred)
    depth = 1.0 / aligned_pred
    return depth, valid_mask

def eval_tae(pred_depth, gt_depth, poses, Ks, masks):
    error_sum = 0.0
    for i in range(len(pred_depth) - 1):
        depth1, depth2 = pred_depth[i], pred_depth[i + 1]
        mask1, mask2   = masks[i], masks[i + 1]

        T_1, T_2 = poses[i], poses[i + 1]
        try:
            T_2_inv = torch.linalg.inv(T_2)
        except torch._C._LinAlgError:
            T_2_inv = torch.linalg.pinv(T_2)

        T_2_1 = T_2_inv @ T_1
        R_2_1 = T_2_1[:3, :3]
        t_2_1 = T_2_1[:3, 3]

        K = Ks[i]
        if K.dim() == 1 and K.numel() == 9:
            K = K.view(3, 3)

        error1 = tae_torch(depth1, depth2, R_2_1, t_2_1, K, mask2)

        try:
            T_1_2 = torch.linalg.inv(T_2_1)
        except torch._C._LinAlgError:
            T_1_2 = torch.linalg.pinv(T_2_1)

        R_1_2 = T_1_2[:3, :3]
        t_1_2 = T_1_2[:3, 3]
        error2 = tae_torch(depth2, depth1, R_1_2, t_1_2, K, mask1)

        error_sum += error1 + error2

    return error_sum / (2 * (len(pred_depth) - 1))

def metric_val(infs, gts, data_name, poses=None, Ks=None):
    gt_depth = gts
    pred_depth, valid_mask = least_square_whole_clip(infs, gts, data_name)

    n = valid_mask.sum((-1, -2))
    valid_frame = (n > 0)
    pred_depth = pred_depth[valid_frame]
    gt_depth   = gt_depth[valid_frame]
    valid_mask = valid_mask[valid_frame]

    absrel = abs_relative_difference(pred_depth, gt_depth, valid_mask)
    delta1 = delta1_acc(pred_depth, gt_depth, valid_mask)

    if poses is not None:
        tae = eval_tae(pred_depth, gt_depth, poses, Ks, valid_mask)
        return absrel, delta1, tae
    else:
        return absrel, delta1

def get_mask(depth_m, min_depth, max_depth):
    return ((depth_m > min_depth) & (depth_m < max_depth)).bool()

def norm_ssi(depth, valid_mask):
    eps = 1e-6
    disparity = torch.zeros_like(depth)
    disparity[valid_mask] = 1.0 / depth[valid_mask]

    B, T, C, H, W = disparity.shape
    disp_flat = disparity.view(B, T, -1)
    mask_flat = valid_mask.view(B, T, -1)

    disp_min = disp_flat.masked_fill(~mask_flat, float('inf')).min(dim=-1)[0]
    disp_max = disp_flat.masked_fill(~mask_flat, float('-inf')).max(dim=-1)[0]
    disp_min = disp_min.view(B, T, 1, 1, 1)
    disp_max = disp_max.view(B, T, 1, 1, 1)

    norm_disp = (disparity - disp_min) / (disp_max - disp_min + eps)
    return norm_disp.masked_fill(~valid_mask, 0.0)

def to_BHW_pred(pred):
    # pred: [B,H,W] or [B,1,H,W] or [B,C,H,W]
    if pred.dim() == 3:
        return pred
    if pred.dim() == 4:
        if pred.size(1) == 1:
            return pred[:, 0]              # [B,H,W]
        else:
            # C>1ì¸ ê²½ìš°(ë“œë¬¼ì§€ë§Œ ë°œìƒ): ì±„ë„ ì¶• í‰ê· ìœ¼ë¡œ ë‹¨ì¼ disparity ìƒì„±
            return pred.mean(dim=1)        # [B,H,W]
    raise ValueError(f"Unexpected pred shape: {pred.shape}")


# ... (í‰ê°€ í•¨ìˆ˜ë“¤: least_square_whole_clip, eval_tae, metric_val, get_mask, norm_ssi) ...

def save_validation_frames(x, y, masks, aligned_disp, save_dir, epoch):
    os.makedirs(save_dir, exist_ok=True)
    T = x.shape[1]
    MEAN = torch.tensor((0.485, 0.456, 0.406), device=x.device).view(3, 1, 1)
    STD  = torch.tensor((0.229, 0.224, 0.225), device=x.device).view(3, 1, 1)
    
    wb_images = []
    for t in range(T):
        # (a) RGB
        rgb_norm = x[0, t]
        rgb_unc  = (rgb_norm * STD + MEAN).clamp(0, 1)
        rgb_np   = (rgb_unc.cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
        Image.fromarray(rgb_np).save(os.path.join(save_dir, f"rgb_{t:02d}.png"))
        
        # (d) Pred Disparity
        pred_frame = aligned_disp[0, t]
        disp_frame = 1.0 / y[0, t].squeeze(0).clamp(min=1e-6)
        valid = masks[0, t].squeeze(0)
        d_vals = disp_frame[valid]
        if d_vals.numel() > 0:
            d_min, d_max = d_vals.min(), d_vals.max()
            norm_pd = ((pred_frame - d_min) / (d_max - d_min + 1e-6)).clamp(0, 1)
            pd_uint8 = (norm_pd.cpu().numpy() * 255).astype(np.uint8)
            pd_rgb = np.stack([pd_uint8] * 3, axis=-1)
            Image.fromarray(pd_rgb).save(os.path.join(save_dir, f"pred_{t:02d}.png"))
            
            wb_images.append(
                wandb.Image(os.path.join(save_dir, f"pred_{t:02d}.png"),
                            caption=f"pred_epoch{epoch}_frame{t:02d}")
            )
    return wb_images

def _detach_cache(cache):
    if cache is None:
        return None
    if isinstance(cache, (list, tuple)):
        return type(cache)(_detach_cache(c) for c in cache)
    if isinstance(cache, dict):
        return {k: _detach_cache(v) for k, v in cache.items()}
    if torch.is_tensor(cache):
        return cache.detach()
    return cache  # unknown type as-is

def model_stream_step(model, x_t, cache=None):
    """
    x_t: [B,1,3,H,W] (single-frame step)
    return: pred_t [B, H, W], new_cache
    """
    # ìš°ì„  ì‹œë„: ìŠ¤íŠ¸ë¦¬ë° ì‹œê·¸ë‹ˆì²˜
    try:
        out = model(x_t, cached_hidden_states=cache, update_cache=True, return_cache=True)
        if isinstance(out, tuple) and len(out) == 2:
            pred_t, new_cache = out
        else:
            # (pred, cache) í˜•íƒœê°€ ì•„ë‹ˆë©´ í˜¸í™˜ ë¶ˆê°€ â†’ í´ë°±
            raise TypeError
        # pred_t: [B,1,H,W] ë¡œ ê°€ì •
        if pred_t.dim() == 4 and pred_t.size(1) == 1:
            pred_t = pred_t[:, 0]  # [B,H,W]
        return pred_t, new_cache
    except TypeError:
        # í´ë°±: ìºì‹œ ë¯¸ì§€ì› ëª¨ë¸ â†’ ëˆ„ì  ìŠ¬ë¼ì´ìŠ¤ë¥¼ ë„£ê³  ë§ˆì§€ë§‰ í”„ë ˆì„ë§Œ ì‚¬ìš©
        # ì£¼ì˜: í•™ìŠµ ì†ë„ëŠ” ëŠë ¤ì§€ì§€ë§Œ train/test ì¡°ê±´ ì •ë ¬ ëª©ì ì€ ë‹¬ì„±
        T_now = x_t.size(1)  # ë³´í†µ 1
        out_full = model(x_t)  # [B,T_now,H,W]
        if out_full.dim() == 4 and out_full.size(1) == T_now:
            pred_t = out_full[:, -1]  # ë§ˆì§€ë§‰ í”„ë ˆì„ë§Œ
        else:
            pred_t = out_full  # [B,H,W] ì¸ ê²½ìš°
        return pred_t, None

def streaming_validate( model, loader, device, data_name, loss_ssi, loss_tgm, ratio_ssi, ratio_tgm, save_vis: bool = False, tag: str = None, epoch: int = None ):
    """
    - ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ê²€ì¦ (1-frame step)
    - save_vis=Trueë©´ ê° ì—í­ë§ˆë‹¤ ê° ë°ì´í„°ì…‹ì˜ 'ì²« ë°°ì¹˜'ë§Œ ì´ë¯¸ì§€ ì €ì¥ + W&B ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    - ë°˜í™˜: avg_loss, avg_absrel, avg_delta1, avg_tae, wb_images(list)
    """
    model.eval()
    total_absrel = 0.0
    total_delta1 = 0.0
    total_tae    = 0.0
    total_loss   = 0.0
    cnt_clip     = 0
    wb_images    = []

    MIN_DISP = 1.0 / 80.0
    MAX_DISP = 1.0 / 0.001

    with torch.no_grad():
        for batch_idx, (x, y, extrinsics, intrinsics) in tqdm(enumerate(loader)):
            x, y = x.to(device), y.to(device)
            extrinsics, intrinsics = extrinsics.to(device), intrinsics.to(device)

            B, T = x.shape[:2]
            preds = []
            mask_list = []   # [B,1,H,W] per frame
            cache = None
            prev_pred = None
            prev_mask = None
            prev_y    = None

            for t in range(T):
                x_t = x[:, t:t+1]  # [B,1,3,H,W]
                pred_t, cache = model_stream_step(model, x_t, cache)
                pred_t = to_BHW_pred(pred_t)             # [B,H,W]
                preds.append(pred_t)

                mask_t = get_mask(y[:, t:t+1], min_depth=1e-3, max_depth=80.0).to(device)  # [B,1,1,H,W]
                mask_list.append(mask_t.squeeze(2))  # â†’ [B,1,H,W]

                # framewise loss(ë¡œê·¸ìš©)
                disp_normed_t = norm_ssi(y[:, t:t+1], mask_t).squeeze(2)   # [B,1,H,W]
                ssi_loss_t = loss_ssi(pred_t.unsqueeze(1), disp_normed_t, mask_t.squeeze(2))
                total_loss += ratio_ssi * ssi_loss_t

                if t > 0:
                    pred_pair = torch.stack([prev_pred, pred_t], dim=1)  # [B,2,H,W]
                    y_pair    = torch.cat([prev_y, y[:, t:t+1]], dim=1)  # [B,2,1,H,W]
                    m_pair    = torch.cat([prev_mask, mask_t], dim=1)    # [B,2,1,H,W]
                    tgm_loss  = loss_tgm(pred_pair, y_pair, m_pair.squeeze(2))
                    total_loss += ratio_tgm * tgm_loss

                prev_pred = pred_t
                prev_mask = mask_t
                prev_y    = y[:, t:t+1]

            pred_seq  = torch.stack(preds, dim=1)               # [B,T,H,W]
            masks_seq = torch.stack(mask_list, dim=1)           # [B,T,1,H,W]

            # metric (í´ë¦½ ë‹¨ìœ„ LS ì •ë ¬ í¬í•¨)
            for b in range(B):
                inf_clip  = pred_seq[b]           # [T,H,W]
                gt_clip   = y[b].squeeze(1)       # [T,H,W]
                pose      = extrinsics[b]
                Kmat      = intrinsics[b]
                absr_dae = metric_val(inf_clip, gt_clip, data_name, pose, Kmat)
                if isinstance(absr_dae, tuple) and len(absr_dae) == 3:
                    absr, d1, tae = absr_dae
                    total_tae += tae
                else:
                    absr, d1 = absr_dae
                total_absrel += absr
                total_delta1 += d1
                cnt_clip     += 1

            # ì‹œê°í™” ì €ì¥ (ì²« ë°°ì¹˜ë§Œ)
            if save_vis and batch_idx == 0 and tag is not None and epoch is not None:
                # í´ë¦½ ë‹¨ìœ„ LSë¡œ ì •ë ¬í•œ disparity (ì‹œê°í™”ìš©)
                Bv, Tv, H, W = pred_seq.shape
                raw_disp = pred_seq.clamp(min=1e-6)              # [B,T,H,W]
                gt_disp  = (1.0 / y.clamp(min=1e-6)).squeeze(2)  # [B,T,H,W]
                m_flat   = masks_seq.squeeze(2).view(Bv, -1).float()
                p_flat   = raw_disp.view(Bv, -1)
                g_flat   = gt_disp.view(Bv, -1)

                A = torch.stack([p_flat, torch.ones_like(p_flat, device=device)], dim=-1)  # [B,P,2]
                A = A * m_flat.unsqueeze(-1)
                b_vec = g_flat.unsqueeze(-1) * m_flat.unsqueeze(-1)
                X = torch.linalg.lstsq(A, b_vec).solution
                a = X[:, 0, 0].view(Bv, 1, 1, 1)
                b = X[:, 1, 0].view(Bv, 1, 1, 1)
                aligned_disp = (raw_disp * a + b).clamp(min=MIN_DISP, max=MAX_DISP)  # [B,T,H,W]

                save_dir = f"outputs/experiment_{experiment}/{tag}/epoch_{epoch}_batch_{batch_idx}"
                wb_images.extend(save_validation_frames(x, y, masks_seq, aligned_disp, save_dir, epoch, batch_idx))

    avg_absrel = total_absrel / max(1, cnt_clip)
    avg_delta1 = total_delta1 / max(1, cnt_clip)
    avg_tae    = total_tae    / max(1, cnt_clip)
    avg_loss   = total_loss   / max(1, cnt_clip)
    return avg_loss, avg_absrel, avg_delta1, avg_tae, wb_images


def batch_ls_scale_shift(pred_disp, gt_disp, mask):
    """
    pred_disp: [B, H, W] or [B,1,H,W] disparity (>= 1e-6)
    gt_disp  : [B, H, W] or [B,1,H,W] disparity
    mask     : [B,1,H,W] bool

    return a_star, b_star with shape [B,1,1,1]
    """
    if pred_disp.dim() == 4 and pred_disp.size(1) == 1:
        p = pred_disp[:, 0]
    else:
        p = pred_disp
    if gt_disp.dim() == 4 and gt_disp.size(1) == 1:
        g = gt_disp[:, 0]
    else:
        g = gt_disp

    B, H, W = p.shape
    m = mask.view(B, -1).float()                      # [B, P]
    p_flat = p.view(B, -1)                            # [B, P]
    g_flat = g.view(B, -1)                            # [B, P]

    A = torch.stack([p_flat, torch.ones_like(p_flat, device=p.device)], dim=-1)  # [B,P,2]
    A = A * m.unsqueeze(-1)
    b_vec = g_flat.unsqueeze(-1) * m.unsqueeze(-1)

    X = torch.linalg.lstsq(A, b_vec).solution        # [B,2,1]
    a_star = X[:, 0, 0].view(B, 1, 1, 1)
    b_star = X[:, 1, 0].view(B, 1, 1, 1)

    # ì•ˆì •ì„±: aëŠ” ì–‘ìˆ˜ë¡œ, ê·¹ë‹¨ì¹˜ í´ë¦¬í•‘
    a_star = a_star.clamp(min=1e-4, max=1e4)
    b_star = b_star.clamp(min=-1e4, max=1e4)
    return a_star, b_star

def ema_update(prev, new, alpha):
    if prev is None:
        return new
    return (1.0 - alpha) * prev + alpha * new

# ... (ë‚˜ë¨¸ì§€ í—¬í¼ í•¨ìˆ˜ë“¤: _detach_cache, model_stream_step, to_BHW_pred) ...

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ train (DDP ë° ê°œì„ ëœ í•™ìŠµ ë¡œì§) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train(args):
    setup_ddp()
    logger = setup_logging(is_main_process())
    device = torch.device(f"cuda:{os.environ['LOCAL_RANK']}")

    OUTPUT_DIR = f"outputs/experiment_{experiment}"
    if is_main_process():
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        logger.info("ğŸ” System Information:")
        logger.info(f"   â€¢ PyTorch version: {torch.__version__}")
        logger.info(f"   â€¢ CUDA available: {torch.cuda.is_available()}")
        logger.info(f"   â€¢ World Size (GPU count): {dist.get_world_size()}")

    # W&B ë¡œê·¸ì¸ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)
    if is_main_process():
        load_dotenv(dotenv_path=".env")
        api_key = os.getenv("WANDB_API_KEY")
        wandb.login(key=api_key, relogin=True)

    with open("configs/config.yaml", "r") as f:
        config = yaml.safe_load(f)
    hyper_params = config["hyper_parameter"]
    
    # (ìˆ˜ì •) ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” ì´ì œ GPUë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì˜ë¯¸
    batch_size = hyper_params["batch_size"] 
    lr, ratio_ssi, ratio_tgm, num_epochs, CLIP_LEN = (
        hyper_params["learning_rate"], hyper_params["ratio_ssi"],
        hyper_params["ratio_tgm"], hyper_params["epochs"], hyper_params["clip_len"]
    )

    if is_main_process():
        run = wandb.init(project="stream_causal_block", entity="Depth-Finder", config=hyper_params)
    
    # â”€â”€ ë°ì´í„° ë¡œë” (DistributedSampler ì ìš©) â”€â”€
    kitti_path = "/workspace/Video-Depth-Anything/datasets/KITTI"
    kitti_train_dataset = KITTIVideoDataset(
        rgb_paths=get_data_list(kitti_path, "kitti", "train", CLIP_LEN)[0],
        depth_paths=get_data_list(kitti_path, "kitti", "train", CLIP_LEN)[1],
        resize_size=350, split="train"
    )
    kitti_val_dataset = KITTIVideoDataset(...) # Val ë°ì´í„°ì…‹ë„ ë™ì¼í•˜ê²Œ ì •ì˜

    # (ìˆ˜ì •) DistributedSampler ì‚¬ìš©
    train_sampler = DistributedSampler(kitti_train_dataset)
    val_sampler = DistributedSampler(kitti_val_dataset, shuffle=False)

    kitti_train_loader = DataLoader(kitti_train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4, pin_memory=True)
    kitti_val_loader = DataLoader(kitti_val_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=4, pin_memory=True)
    
    # ScanNet ë°ì´í„° ë¡œë”ë„ ë™ì¼í•˜ê²Œ DistributedSampler ì ìš© ...

    # â”€â”€ ëª¨ë¸ (DDP ì ìš©) â”€â”€
    model = VideoDepthAnything(...).to(device)
    
    if args.pretrained_ckpt:
        ckpt = torch.load(args.pretrained_ckpt, map_location="cpu")
        state_dict = ckpt.get('model_state_dict', ckpt.get('state_dict', ckpt))
        
        # (ìˆ˜ì •) DDPë¡œ ë˜í•‘í•˜ê¸° ì „ì— ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        # (ìˆ˜ì •) strict=Falseë¡œ ë³€ê²½í•˜ì—¬ ìœ ì—°ì„± í™•ë³´
        model.load_state_dict(state_dict, strict=False) 
        if is_main_process():
            logger.info("âœ… Pretrained weights loaded successfully (strict=False)")
    
    for p in model.pretrained.parameters(): p.requires_grad = False
    for p in model.head.parameters(): p.requires_grad = True

    # (ìˆ˜ì •) DDPë¡œ ëª¨ë¸ ë˜í•‘
    model = DDP(model, device_ids=[int(os.environ["LOCAL_RANK"])], find_unused_parameters=True)

    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)
    scaler = GradScaler()
    
    loss_tgm = LossTGMVector(diff_depth_th=0.05).to(device)
    loss_ssi = Loss_ssi_basic().to(device)
    
    best_model_path   = os.path.join(OUTPUT_DIR, "best_model.pth")
    latest_model_path = os.path.join(OUTPUT_DIR, "latest_model.pth")

    if is_main_process():
        wandb.watch(model, log="all")

    # â”€â”€ í•™ìŠµ ë£¨í”„ (ìˆ˜ì •ëœ í•™ìŠµ ëª©í‘œ ì ìš©) â”€â”€
    for epoch in range(num_epochs):
        model.train()
        train_sampler.set_epoch(epoch) # (ìˆ˜ì •) ë§¤ ì—í­ë§ˆë‹¤ ìƒ˜í”ŒëŸ¬ ì…”í”Œë§
        
        pbar = tqdm(kitti_train_loader, desc=f"Epoch {epoch} Training", disable=not is_main_process())
        
        for batch_idx, (x, y) in enumerate(pbar):
            x, y = x.to(device), y.to(device)
            B, T = x.shape[:2]
            
            cache = None
            prev_pred_norm = None
            prev_gt_norm = None

            optimizer.zero_grad(set_to_none=True)
            
            # --- (ìˆ˜ì •) ìƒˆë¡œìš´ í•™ìŠµ ëª©í‘œ ---
            # Gradient Accumulationì„ ìœ„í•´ ë£¨í”„ë¥¼ í•œë²ˆ ë” ë•ë‹ˆë‹¤.
            accum_loss = 0.0
            for t in range(T):
                x_t = x[:, t:t+1]
                y_t = y[:, t:t+1]
                mask_t = get_mask(y_t, 1e-3, 80.0)

                with autocast():
                    # 1. ëª¨ë¸ì´ ì§ì ‘ ì •ê·œí™”ëœ Disparityë¥¼ ì˜ˆì¸¡
                    pred_t_norm, cache = model_stream_step(model, x_t, cache)
                    pred_t_norm = to_BHW_pred(pred_t_norm)

                    # 2. GTë¥¼ ë™ì¼í•˜ê²Œ ì •ê·œí™”
                    gt_norm_t = norm_ssi(y_t, mask_t).squeeze(1) # [B,1,H,W]

                    # 3. ì •ê·œí™”ëœ ê°’ë“¤ë¡œ ì†ì‹¤ ê³„ì‚°
                    ssi_loss_t = loss_ssi(pred_t_norm.unsqueeze(1), gt_norm_t, mask_t.squeeze(2))
                    
                    if t > 0:
                        # TGMì€ ì •ê·œí™”ëœ ê°’ ì‚¬ì´ì—ì„œ ê³„ì‚° (ë˜ëŠ” ê¹Šì´ë¡œ ë³€í™˜ í›„ ê³„ì‚°)
                        # ì—¬ê¸°ì„œëŠ” ì •ê·œí™”ëœ ê³µê°„ì—ì„œ ê³„ì‚°í•œë‹¤ê³  ê°€ì •
                        pred_pair = torch.stack([prev_pred_norm, pred_t_norm], dim=1)
                        gt_pair_norm = torch.stack([prev_gt_norm, gt_norm_t.squeeze(1)], dim=1)
                        # TGM LossëŠ” ê¹Šì´ ìŠ¤ì¼€ì¼ì´ ì¤‘ìš”í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê°œë…ì  í‘œí˜„ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
                        # ì‹¤ì œ ì ìš© ì‹œì—ëŠ” ì´ ë¶€ë¶„ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì‹¤í—˜ì´ í•„ìš”í•©ë‹ˆë‹¤.
                        # ì—¬ê¸°ì„œëŠ” SSIì— ì§‘ì¤‘í•˜ê¸° ìœ„í•´ TGM ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ê°€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        tgm_loss = torch.tensor(0.0, device=device)
                    else:
                        tgm_loss = torch.tensor(0.0, device=device)

                    loss = (ratio_ssi * ssi_loss_t + ratio_tgm * tgm_loss)

                # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•´ ì†ì‹¤ì„ ìŠ¤ì¼€ì¼ë§
                scaler.scale(loss).backward()
                accum_loss += loss.item()

                # ìƒíƒœ ì—…ë°ì´íŠ¸ (detach í•„ìˆ˜)
                cache = _detach_cache(cache)
                prev_pred_norm = pred_t_norm.detach()
                prev_gt_norm = gt_norm_t.squeeze(1).detach()
            
            # ëˆ„ì ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¡œ ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            scaler.step(optimizer)
            scaler.update()

            if is_main_process():
                pbar.set_postfix(loss=f"{accum_loss:.4f}")

        # â”€â”€ ê²€ì¦ ë£¨í”„ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ìˆ˜í–‰) â”€â”€
        if is_main_process():
            model.eval()
            with torch.no_grad():
                # streaming_validate í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦
                # ì´ í•¨ìˆ˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í´ë¦½ ë‹¨ìœ„ LS ì •ë ¬ í›„ ë©”íŠ¸ë¦­ ê³„ì‚°
                kitti_val_loss, kitti_absrel, kitti_delta1, kitti_tae, kitti_wb_images = streaming_validate(
                    model, kitti_val_loader, device, "kitti",
                    loss_ssi, loss_tgm, ratio_ssi, ratio_tgm,
                    save_vis=True, tag="vkitti", epoch=epoch
                )

                # â”€â”€ ScanNet(stream) â”€â”€
                scannet_val_loss, scannet_absrel, scannet_delta1, scannet_tae, scannet_wb_images = streaming_validate(
                    model, scannet_val_loader, device, "scannet",
                    loss_ssi, loss_tgm, ratio_ssi, ratio_tgm,
                    save_vis=True, tag="scannet", epoch=epoch
                )

            # â”€â”€ W&B ë¡œê¹… â”€â”€
            wandb.log({
                # "train_loss_stream": avg_kitti_train_loss,
                "kitti_stream_loss":  kitti_val_loss,
                "kitti_stream_absrel": kitti_absrel,
                "kitti_stream_delta1": kitti_delta1,
                "kitti_stream_tae":    kitti_tae,
                "vkitti_pred_disparity": kitti_wb_images,     # â† ì´ë¯¸ì§€ ì—…ë¡œë“œ
                "scannet_stream_loss":  scannet_val_loss,
                "scannet_stream_absrel": scannet_absrel,
                "scannet_stream_delta1": scannet_delta1,
                "scannet_stream_tae":    scannet_tae,
                "scannet_pred_disparity": scannet_wb_images,  # â† ì´ë¯¸ì§€ ì—…ë¡œë“œ
                "epoch": epoch,
            })
            del kitti_wb_images
            del scannet_wb_images
        
            # â”€â”€ ëª¨ë¸ ì €ì¥ (ScanNet AbsRel ê¸°ì¤€) â”€â”€
            if scannet_absrel < best_scannet_absrel:
                best_scannet_absrel = scannet_absrel
                best_epoch = epoch

                torch.save({
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "scheduler_state_dict": scheduler.state_dict(),
                    "best_scannet_absrel": best_scannet_absrel,
                    "scannet_val_loss": scannet_val_loss,
                    "scannet_delta1": scannet_delta1,
                    "scannet_tae": scannet_tae,
                    "config": hyper_params,
                }, best_model_path)

                logger.info(f"ğŸ† Best model saved! Epoch {epoch}, ScanNet AbsRel: {best_scannet_absrel:.4f}")
                wandb.log({
                    "best_scannet_absrel": best_scannet_absrel,
                    "best_epoch": epoch,
                    "model_improved": True,
                    "epoch": epoch,
                })
            else:
                wandb.log({"model_improved": False, "epoch": epoch})

            # latest ì €ì¥
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict(),
                "current_scannet_absrel": scannet_absrel,
                "config": hyper_params,
            }, latest_model_path)
            logger.info(f"ğŸ“ Latest model saved to {latest_model_path}")

        scheduler.step()
    
    if is_main_process():
        run.finish()
    cleanup_ddp()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--pretrained_ckpt", type=str, default="./checkpoints/video_depth_anything_vits.pth")
    # (ìˆ˜ì •) DDPë¥¼ ìœ„í•œ ì¸ìë“¤ (torch.distributed.launchê°€ ìë™ìœ¼ë¡œ ì±„ì›Œì¤Œ)
    parser.add_argument('--local_rank', type=int, default=-1, help='local rank for distributed training')
    args = parser.parse_args()
    train(args)